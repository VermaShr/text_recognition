{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import BasicLSTMCell\n",
    "\n",
    "import pickle\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from .decoding import get_words_from_chars\n",
    "# from .config import Params, CONST\n",
    "# from src.model import crnn_fn\n",
    "# from src.data_handler import data_loader\n",
    "# from src.data_handler import preprocess_image_for_prediction\n",
    "\n",
    "# from src.config import Params, Alphabet, import_params_from_json\n",
    "\n",
    "csv_files_train = \"../data/train.csv\"\n",
    "csv_files_eval = \"../data/valid.csv\"\n",
    "output_model_dir = \"./estimator\"\n",
    "if not os.path.isdir(output_model_dir):\n",
    "    os.mkdir(output_model_dir)\n",
    "\n",
    "\n",
    "n_epochs = 1\n",
    "gpu = \"\" # help=\"GPU 0,1 or '' \", default=''\n",
    "\n",
    "train_batch_size=64\n",
    "eval_batch_size=64\n",
    "learning_rate=1e-3  # 1e-3 recommended\n",
    "learning_decay_rate=0.95\n",
    "learning_decay_steps=5000\n",
    "evaluate_every_epoch=5\n",
    "save_interval=5e3\n",
    "input_shape=(129, 369)\n",
    "optimizer='adam'\n",
    "digits_only=False\n",
    "# alphabet=\" !\\\"#&'()*+,-./0123456789:;<=>?ABCDEFGHIJKLMNOPQRSTUVWXY[]_abcdefghijklmnopqrstuvwxyz|~\"\n",
    "alphabet=\"abcdefghijklmnopqrstuvwxyz\"\n",
    "alphabet_decoding='same'\n",
    "alphabet_codes = list(range(len(alphabet)))\n",
    "n_classes = len(alphabet)\n",
    "csv_delimiter='\\t'\n",
    "keep_prob_dropout = 0.7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of tensor graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# needed for quickly making convolutional layers\n",
    "def weightVar(shape, mean=0.0, stddev=0.02, name='weights'):\n",
    "    init_w = tf.truncated_normal(shape=shape, mean=mean, stddev=stddev)\n",
    "    return tf.Variable(init_w, name=name)\n",
    "\n",
    "\n",
    "def biasVar(shape, value=0.0, name='bias'):\n",
    "    init_b = tf.constant(value=value, shape=shape)\n",
    "    return tf.Variable(init_b, name=name)\n",
    "\n",
    "\n",
    "def conv2d(input, filter, strides=[1, 1, 1, 1], padding='SAME', name=None):\n",
    "    return tf.nn.conv2d(input, filter, strides=strides, padding=padding, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input_tensor = features['images']\n",
    "input_tensor = tf.placeholder(tf.float32, [None, input_shape[0], input_shape[1], 1])\n",
    "labels = tf.placeholder(tf.string, [None])\n",
    "is_training = True\n",
    "\n",
    "if input_tensor.shape[-1] == 1:\n",
    "    input_channels = 1\n",
    "elif input_tensor.shape[-1] == 3:\n",
    "    input_channels = 3\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('deep_cnn'):\n",
    "    # conv1 - maxPool2x2\n",
    "    with tf.variable_scope('layer1'):\n",
    "        W = weightVar([3, 3, input_channels, 64])\n",
    "        b = biasVar([64])\n",
    "        conv = conv2d(input_tensor, W, name='conv')\n",
    "        out = tf.nn.bias_add(conv, b)\n",
    "        conv1 = tf.nn.relu(out)\n",
    "        pool1 = tf.nn.max_pool(conv1, [1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                               padding='SAME', name='pool')\n",
    "\n",
    "    # conv2 - maxPool 2x2\n",
    "    with tf.variable_scope('layer2'):\n",
    "        W = weightVar([3, 3, 64, 128])\n",
    "        b = biasVar([128])\n",
    "        conv = conv2d(pool1, W)\n",
    "        out = tf.nn.bias_add(conv, b)\n",
    "        conv2 = tf.nn.relu(out)\n",
    "        pool2 = tf.nn.max_pool(conv2, [1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                               padding='SAME', name='pool1')\n",
    "\n",
    "    # conv3 - w/batch-norm (as source code, not paper)\n",
    "    with tf.variable_scope('layer3'):\n",
    "        W = weightVar([3, 3, 128, 256])\n",
    "        b = biasVar([256])\n",
    "        conv = conv2d(pool2, W)\n",
    "        out = tf.nn.bias_add(conv, b)\n",
    "        b_norm = tf.layers.batch_normalization(out, axis=-1,\n",
    "                                               training=is_training, name='batch-norm')\n",
    "        conv3 = tf.nn.relu(b_norm, name='ReLU')\n",
    "\n",
    "    # conv4 - maxPool 2x1\n",
    "    with tf.variable_scope('layer4'):\n",
    "        W = weightVar([3, 3, 256, 256])\n",
    "        b = biasVar([256])\n",
    "        conv = conv2d(conv3, W)\n",
    "        out = tf.nn.bias_add(conv, b)\n",
    "        conv4 = tf.nn.relu(out)\n",
    "        pool4 = tf.nn.max_pool(conv4, [1, 2, 2, 1], strides=[1, 2, 1, 1],\n",
    "                               padding='SAME', name='pool4')\n",
    "\n",
    "    # conv5 - w/batch-norm\n",
    "    with tf.variable_scope('layer5'):\n",
    "        W = weightVar([3, 3, 256, 512])\n",
    "        b = biasVar([512])\n",
    "        conv = conv2d(pool4, W)\n",
    "        out = tf.nn.bias_add(conv, b)\n",
    "        b_norm = tf.layers.batch_normalization(out, axis=-1,\n",
    "                                               training=is_training, name='batch-norm')\n",
    "        conv5 = tf.nn.relu(b_norm)\n",
    "\n",
    "    # conv6 - maxPool 2x1 (as source code, not paper)\n",
    "    with tf.variable_scope('layer6'):\n",
    "        W = weightVar([3, 3, 512, 512])\n",
    "        b = biasVar([512])\n",
    "        conv = conv2d(conv5, W)\n",
    "        out = tf.nn.bias_add(conv, b)\n",
    "        conv6 = tf.nn.relu(out)\n",
    "        pool6 = tf.nn.max_pool(conv6, [1, 2, 2, 1], strides=[1, 2, 1, 1],\n",
    "                               padding='SAME', name='pool6')\n",
    "\n",
    "    # conv 7 - w/batch-norm (as source code, not paper)\n",
    "    with tf.variable_scope('layer7'):\n",
    "        W = weightVar([2, 2, 512, 512])\n",
    "        b = biasVar([512])\n",
    "        conv = conv2d(pool6, W, padding='VALID')\n",
    "        out = tf.nn.bias_add(conv, b)\n",
    "        b_norm = tf.layers.batch_normalization(out, axis=-1,\n",
    "                                               training=is_training, name='batch-norm')\n",
    "        conv7 = tf.nn.relu(b_norm)\n",
    "\n",
    "    # reshape output\n",
    "    with tf.variable_scope('Reshaping_cnn'):\n",
    "        shape = conv7.get_shape().as_list()  # [batch, height, width, features]\n",
    "        shape_tens = tf.shape(conv7)\n",
    "        transposed = tf.transpose(conv7, perm=[0, 2, 1, 3],\n",
    "                                  name='transposed')  # [batch, width, height, features]\n",
    "        conv_out = tf.reshape(transposed, [shape_tens[0], -1, shape[1] * shape[3]],\n",
    "                                   name='reshaped')  # [batch, width, height x features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# logprob, raw_pred = deep_bidirectional_lstm(conv, params=parameters, summaries=False)\n",
    "\n",
    "# def deep_bidirectional_lstm(inputs: tf.Tensor, params: Params, summaries: bool=True) -> tf.Tensor:\n",
    "# Prepare data shape to match `bidirectional_rnn` function requirements\n",
    "# Current data input shape: (batch_size, n_steps, n_input) \"(batch, time, height)\"\n",
    "\n",
    "list_n_hidden = [256, 256]\n",
    "\n",
    "with tf.name_scope('deep_bidirectional_lstm'):\n",
    "    # Forward direction cells\n",
    "    fw_cell_list = [BasicLSTMCell(nh, forget_bias=1.0) for nh in list_n_hidden]\n",
    "    # Backward direction cells\n",
    "    bw_cell_list = [BasicLSTMCell(nh, forget_bias=1.0) for nh in list_n_hidden]\n",
    "\n",
    "    lstm_net, _, _ = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(fw_cell_list,\n",
    "                                                                    bw_cell_list,\n",
    "                                                                    conv_out, # THE INPUT\n",
    "                                                                    dtype=tf.float32\n",
    "                                                                    )\n",
    "\n",
    "    # Dropout layer\n",
    "    lstm_net = tf.nn.dropout(lstm_net, keep_prob=keep_prob_dropout)\n",
    "\n",
    "    with tf.variable_scope('Reshaping_rnn'):\n",
    "        shape = lstm_net.get_shape().as_list()  # [batch, width, 2*n_hidden]\n",
    "        rnn_reshaped = tf.reshape(lstm_net, [-1, shape[-1]])  # [batch x width, 2*n_hidden]\n",
    "\n",
    "    with tf.variable_scope('fully_connected'):\n",
    "        W = weightVar([list_n_hidden[-1]*2, n_classes])\n",
    "        b = biasVar([n_classes])\n",
    "        fc_out = tf.nn.bias_add(tf.matmul(rnn_reshaped, W), b)\n",
    "\n",
    "    shape_tens = tf.shape(lstm_net)\n",
    "    lstm_out = tf.reshape(fc_out, [shape_tens[0], -1, n_classes], name='reshape_out')  # [batch, width, n_classes]\n",
    "\n",
    "    raw_pred = tf.argmax(tf.nn.softmax(lstm_out), axis=2, name='raw_prediction')\n",
    "\n",
    "    # Swap batch and time axis\n",
    "    lstm_out = tf.transpose(lstm_out, [1, 0, 2], name='transpose_time_major')  # [width(time), batch, n_classes]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTC loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up for loss and training\n",
    "\n",
    "# Compute seq_len from image width\n",
    "n_pools = 2*2  # 2x2 pooling in dimension W on layer 1 and 2\n",
    "seq_len_inputs = tf.divide([input_shape[1]]*train_batch_size, n_pools,\n",
    "                           name='seq_len_input_op') - 1\n",
    "\n",
    "predictions_dict = {'prob': lstm_out,\n",
    "                    'raw_predictions': raw_pred,\n",
    "                    }\n",
    "\n",
    "\n",
    "# Get keys (letters) and values (integer stand ins for letters)\n",
    "# Alphabet and codes\n",
    "keys = [c for c in alphabet] # the letters themselves\n",
    "values = alphabet_codes # integer representations\n",
    "\n",
    "\n",
    "# Create non-string labels from the keys and values above\n",
    "# Convert string label to code label\n",
    "with tf.name_scope('str2code_conversion'):\n",
    "    table_str2int = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(keys, values), -1)\n",
    "    splited = tf.string_split(labels, delimiter='')  # TODO change string split to utf8 split in next tf version\n",
    "    codes = table_str2int.lookup(splited.values)\n",
    "    sparse_code_target = tf.SparseTensor(splited.indices, codes, splited.dense_shape)\n",
    "\n",
    "seq_lengths_labels = tf.bincount(tf.cast(sparse_code_target.indices[:, 0], tf.int32),\n",
    "                                 minlength=tf.shape(predictions_dict['prob'])[1])\n",
    "\n",
    "\n",
    "# Use ctc loss on probabilities from lstm output\n",
    "# Loss\n",
    "# ----\n",
    "# >>> Cannot have longer labels than predictions -> error\n",
    "with tf.control_dependencies([tf.less_equal(sparse_code_target.dense_shape[1], tf.reduce_max(tf.cast(seq_len_inputs, tf.int64)))]):\n",
    "    loss_ctc = tf.nn.ctc_loss(labels=sparse_code_target,\n",
    "                              inputs=predictions_dict['prob'],\n",
    "                              sequence_length=tf.cast(seq_len_inputs, tf.int32),\n",
    "                              preprocess_collapse_repeated=False,\n",
    "                              ctc_merge_repeated=True,\n",
    "                              ignore_longer_outputs_than_inputs=True,  # returns zero gradient in case it happens -> ema loss = NaN\n",
    "                              time_major=True)\n",
    "    loss_ctc = tf.reduce_mean(loss_ctc)\n",
    "    loss_ctc = tf.Print(loss_ctc, [loss_ctc], message='* Loss : ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTC decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_words_from_chars(characters_list: List[str], sequence_lengths: List[int], name='chars_conversion'):\n",
    "    with tf.name_scope(name=name):\n",
    "        def join_charcaters_fn(coords):\n",
    "            return tf.reduce_join(characters_list[coords[0]:coords[1]])\n",
    "\n",
    "        def coords_several_sequences():\n",
    "            end_coords = tf.cumsum(sequence_lengths)\n",
    "            start_coords = tf.concat([[0], end_coords[:-1]], axis=0)\n",
    "            coords = tf.stack([start_coords, end_coords], axis=1)\n",
    "            coords = tf.cast(coords, dtype=tf.int32)\n",
    "            return tf.map_fn(join_charcaters_fn, coords, dtype=tf.string)\n",
    "\n",
    "        def coords_single_sequence():\n",
    "            return tf.reduce_join(characters_list, keep_dims=True)\n",
    "\n",
    "        words = tf.cond(tf.shape(sequence_lengths)[0] > 1,\n",
    "                        true_fn=lambda: coords_several_sequences(),\n",
    "                        false_fn=lambda: coords_single_sequence())\n",
    "\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('code2str_conversion'):\n",
    "    keys = tf.cast(alphabet_codes, tf.int64)\n",
    "    values = [c for c in alphabet]\n",
    "\n",
    "    table_int2str = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(keys, values), '?')\n",
    "\n",
    "    sparse_code_pred, log_probability = tf.nn.ctc_beam_search_decoder(predictions_dict['prob'],\n",
    "                                                                      sequence_length=tf.cast(seq_len_inputs, tf.int32),\n",
    "                                                                      merge_repeated=False,\n",
    "                                                                      beam_width=100,\n",
    "                                                                      top_paths=2)\n",
    "    # Score\n",
    "    predictions_dict['score'] = tf.subtract(log_probability[:, 0], log_probability[:, 1])\n",
    "    # around 10.0 -> seems pretty sure, less than 5.0 bit unsure, some errors/challenging images\n",
    "    sparse_code_pred = sparse_code_pred[0]\n",
    "\n",
    "    sequence_lengths_pred = tf.bincount(tf.cast(sparse_code_pred.indices[:, 0], tf.int32),\n",
    "                                        minlength=tf.shape(predictions_dict['prob'])[1])\n",
    "\n",
    "    pred_chars = table_int2str.lookup(sparse_code_pred)\n",
    "    predictions_dict['words'] = get_words_from_chars(pred_chars.values, sequence_lengths=sequence_lengths_pred)\n",
    "\n",
    "    tf.summary.text('predicted_words', predictions_dict['words'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('evaluation'):\n",
    "    CER = tf.metrics.mean(tf.edit_distance(sparse_code_pred, tf.cast(sparse_code_target, dtype=tf.int64)), name='CER')\n",
    "    CER = tf.reduce_mean(tf.edit_distance(sparse_code_pred, tf.cast(sparse_code_target, dtype=tf.int64)), name='CER')\n",
    "\n",
    "    # Convert label codes to decoding alphabet to compare predicted and groundtrouth words\n",
    "    target_chars = table_int2str.lookup(tf.cast(sparse_code_target, tf.int64))\n",
    "    target_words = get_words_from_chars(target_chars.values, seq_lengths_labels)\n",
    "    accuracy = tf.metrics.accuracy(target_words, predictions_dict['words'], name='accuracy')\n",
    "\n",
    "    eval_metric_ops = {\n",
    "                       'eval/accuracy': accuracy,\n",
    "                       'eval/CER': CER,\n",
    "                       }\n",
    "    CER = tf.Print(CER, [CER], message='-- CER : ')\n",
    "    accuracy = tf.Print(accuracy, [accuracy], message='-- Accuracy : ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the learning rate as well as a moving average\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "# # Create an ExponentialMovingAverage object\n",
    "ema = tf.train.ExponentialMovingAverage(decay=0.99, num_updates=global_step, zero_debias=True)\n",
    "# Create the shadow variables, and add op to maintain moving averages\n",
    "maintain_averages_op = ema.apply([loss_ctc])\n",
    "loss_ema = ema.average(loss_ctc)\n",
    "\n",
    "# Train op\n",
    "# --------\n",
    "learning_rate = tf.train.exponential_decay(learning_rate, global_step,\n",
    "                                           learning_decay_steps, learning_decay_rate,\n",
    "                                           staircase=True)\n",
    "\n",
    "# Set up optimizer\n",
    "if optimizer == 'ada':\n",
    "    optimizer = tf.train.AdadeltaOptimizer(learning_rate)\n",
    "elif optimizer == 'adam':\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate, beta1=0.5)\n",
    "elif optimizer == 'rms':\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "opt_op = optimizer.minimize(loss_ctc, global_step=global_step)\n",
    "with tf.control_dependencies(update_ops + [opt_op]):\n",
    "    train_op = tf.group(maintain_averages_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the details to make all images the same size\n",
    "\n",
    "with open(csv_files_train, \"r\") as f:\n",
    "    f_lines = f.read().splitlines()[1:]\n",
    "    filenames = [l.split(\"\\t\")[0] for l in f_lines]\n",
    "    label_list = [l.split(\"\\t\")[1] for l in f_lines]\n",
    "\n",
    "datasize = len(label_list)\n",
    "filenames = tf.constant(filenames)\n",
    "label_list = tf.constant(label_list)\n",
    "    \n",
    "def _parse_function(filename, label):\n",
    "    image_string = tf.read_file(filename)\n",
    "    image_decoded = tf.image.decode_png(image_string, channels=1)\n",
    "    image_resized = tf.image.resize_image_with_crop_or_pad(image_decoded,\n",
    "                                                           input_shape[0],\n",
    "                                                           input_shape[1])\n",
    "    return image_resized, label\n",
    "    \n",
    "dataset = tf.data.Dataset.from_tensor_slices((filenames, label_list))\n",
    "dataset = dataset.map(_parse_function)\n",
    "dataset = dataset.batch(train_batch_size)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_batch = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "batch: 0, CER: 15.137815475463867, accuracy: [ 0.  0.], loss: 274.38116455078125\n",
      "batch: 1, CER: 3.7583718299865723, accuracy: [ 0.  0.], loss: 142.14459228515625\n",
      "batch: 2, CER: 0.9717261791229248, accuracy: [ 0.  0.], loss: 45.433021545410156\n",
      "batch: 3, CER: 1.0, accuracy: [ 0.  0.], loss: 18.300533294677734\n",
      "batch: 4, CER: 1.0, accuracy: [ 0.  0.], loss: 14.789356231689453\n",
      "batch: 5, CER: 1.0, accuracy: [ 0.  0.], loss: 15.94019603729248\n",
      "batch: 6, CER: 1.0, accuracy: [ 0.  0.], loss: 13.763711929321289\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-a262132b7c21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m             _, cer, acc, loss, preddict, scp, pc = sess.run([train_op, CER, accuracy, loss_ctc, predictions_dict,\n\u001b[0;32m     18\u001b[0m                                                           sparse_code_pred, pred_chars],\n\u001b[1;32m---> 19\u001b[1;33m                          feed_dict={input_tensor: input_tensor_b, labels: labels_b})\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreddict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_model_dir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/preds.pkl\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_model_dir\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/model.ckpt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\danny\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\danny\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\danny\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\danny\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\danny\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "#     saver.restore(sess, output_model_dir+\"/model0.ckpt\")\n",
    "    \n",
    "    # writer = tf.summary.FileWriter('./my_graph/LogRegNormal', sess.graph)\n",
    "    n_batches = int(datasize / train_batch_size)\n",
    "    for i in range(n_epochs):\n",
    "        print(\"Starting epoch\", i)\n",
    "        sess.run(iterator.initializer)\n",
    "        for j in range(n_batches):\n",
    "            input_tensor_b, labels_b = sess.run(next_batch)\n",
    "            _, cer, acc, loss, preddict, scp, pc = sess.run([train_op, CER, accuracy, loss_ctc, predictions_dict,\n",
    "                                                          sparse_code_pred, pred_chars],\n",
    "                         feed_dict={input_tensor: input_tensor_b, labels: labels_b})\n",
    "            pickle.dump([cer, acc, loss, preddict, scp, pc], open(output_model_dir+\"/preds.pkl\", \"wb\"))\n",
    "            saver.save(sess, output_model_dir+\"/model.ckpt\")\n",
    "            \n",
    "            print('batch: {0}, CER: {1}, accuracy: {2}, loss: {3}'.format(j, cer, acc, loss))\n",
    "\n",
    "    print('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "    print('Optimization Finished!') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try getting actual predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "HandleElementToSlice Cannot copy slice: number of elements does not match.  Shapes are: [element]: [78,166,1], [parent slice]: [50,70,1]\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,?,1], [?]], output_types=[DT_UINT8, DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\n\nCaused by op 'IteratorGetNext', defined at:\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-b7389daf3f17>\", line 20, in <module>\n    next_batch = iterator.get_next()\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 259, in get_next\n    name=name))\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\", line 705, in iterator_get_next\n    output_shapes=output_shapes, name=name)\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInternalError (see above for traceback): HandleElementToSlice Cannot copy slice: number of elements does not match.  Shapes are: [element]: [78,166,1], [parent slice]: [50,70,1]\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,?,1], [?]], output_types=[DT_UINT8, DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\danny\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\danny\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\danny\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    474\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: HandleElementToSlice Cannot copy slice: number of elements does not match.  Shapes are: [element]: [78,166,1], [parent slice]: [50,70,1]\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,?,1], [?]], output_types=[DT_UINT8, DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-0158ad0733d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0minput_tensor_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             _, cer, acc, loss, preddict, scp, pc = sess.run([train_op, CER, accuracy, loss_ctc, predictions_dict,\n\u001b[0;32m     17\u001b[0m                                                           sparse_code_pred, pred_chars],\n",
      "\u001b[1;32mC:\\Users\\danny\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\danny\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\danny\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\danny\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1334\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1335\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1336\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1338\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: HandleElementToSlice Cannot copy slice: number of elements does not match.  Shapes are: [element]: [78,166,1], [parent slice]: [50,70,1]\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,?,1], [?]], output_types=[DT_UINT8, DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\n\nCaused by op 'IteratorGetNext', defined at:\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-b7389daf3f17>\", line 20, in <module>\n    next_batch = iterator.get_next()\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 259, in get_next\n    name=name))\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\", line 705, in iterator_get_next\n    output_shapes=output_shapes, name=name)\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\danny\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInternalError (see above for traceback): HandleElementToSlice Cannot copy slice: number of elements does not match.  Shapes are: [element]: [78,166,1], [parent slice]: [50,70,1]\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,?,?,1], [?]], output_types=[DT_UINT8, DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # writer = tf.summary.FileWriter('./my_graph/LogRegNormal', sess.graph)\n",
    "    n_batches = int(datasize / train_batch_size)\n",
    "    for i in range(n_epochs):\n",
    "        print(\"Starting epoch\", i)\n",
    "        sess.run(iterator.initializer)\n",
    "        for j in range(n_batches):\n",
    "            input_tensor_b, labels_b = sess.run(next_batch)\n",
    "            _, cer, acc, loss, preddict, scp, pc = sess.run([train_op, CER, accuracy, loss_ctc, predictions_dict,\n",
    "                                                          sparse_code_pred, pred_chars],\n",
    "                         feed_dict={input_tensor: input_tensor_b, labels: labels_b})\n",
    "            \n",
    "            pickle.dump([cer, acc, loss, preddict, scp, pc], open(output_model_dir+\"/preds2.pkl\", \"wb\"))\n",
    "            print('batch: {0}, CER: {1}, accuracy: {2}, loss: {3}'.format(j, cer, acc, loss))\n",
    "            if j > 0: break # for now don't do all of them\n",
    "\n",
    "    print('Total time: {0} seconds'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.67922795, -0.5718289 , -1.10647857, ..., -0.99688321,\n",
       "         -0.70123678,  4.19806194],\n",
       "        [-0.50024879, -0.38692954, -0.95941275, ..., -0.94622153,\n",
       "         -0.72991818,  4.37159634],\n",
       "        [-0.28581473, -0.81606793, -1.04619241, ..., -0.69693077,\n",
       "         -0.83407879,  4.11093855],\n",
       "        ..., \n",
       "        [-0.4161678 , -0.74003816, -1.32267749, ..., -1.1095233 ,\n",
       "         -0.76497471,  4.13257217],\n",
       "        [-0.33939072, -0.84367967, -1.13953483, ..., -0.75803155,\n",
       "         -1.05144715,  4.43799496],\n",
       "        [-0.72410357, -0.61503845, -1.38453233, ..., -1.03141761,\n",
       "         -0.71325225,  4.43202782]],\n",
       "\n",
       "       [[-0.49097443, -0.74971175, -1.13442743, ..., -1.10337138,\n",
       "         -1.02899468,  5.36590624],\n",
       "        [-0.47185379, -0.76773828, -1.09975898, ..., -1.11401272,\n",
       "         -0.87385851,  4.98149586],\n",
       "        [-0.55431497, -0.83526278, -1.1897701 , ..., -1.44856119,\n",
       "         -1.23961306,  5.01430225],\n",
       "        ..., \n",
       "        [-0.32729244, -0.99773765, -1.38073993, ..., -1.34837103,\n",
       "         -0.90211076,  5.42542505],\n",
       "        [-0.45432925, -0.69276339, -1.63143289, ..., -1.21129358,\n",
       "         -1.10758114,  5.23587704],\n",
       "        [-0.43772912, -0.49126506, -1.17866051, ..., -1.10607636,\n",
       "         -1.09435034,  4.80843878]],\n",
       "\n",
       "       [[-0.42200825, -0.79310381, -1.35094392, ..., -1.32764816,\n",
       "         -1.03101361,  5.88902378],\n",
       "        [-0.24003069, -1.00411308, -1.22252142, ..., -1.45639753,\n",
       "         -0.93365473,  5.84848881],\n",
       "        [-0.41151798, -0.99152124, -1.41903317, ..., -1.43467951,\n",
       "         -0.86298651,  5.68479538],\n",
       "        ..., \n",
       "        [-0.65301746, -0.69572115, -1.28494287, ..., -1.35441792,\n",
       "         -1.11878252,  5.85119438],\n",
       "        [-0.61656648, -1.03018284, -1.28289604, ..., -1.63859046,\n",
       "         -0.87785619,  5.76509142],\n",
       "        [-0.44338083, -0.58808798, -1.27896917, ..., -1.39021134,\n",
       "         -0.79547685,  4.87800598]],\n",
       "\n",
       "       ..., \n",
       "       [[-0.57310879, -0.80627841, -0.85502237, ..., -1.28792477,\n",
       "         -1.16001737,  5.72758865],\n",
       "        [-0.32902321, -0.99091661, -1.33179271, ..., -1.40301096,\n",
       "         -0.96159744,  5.56378126],\n",
       "        [-0.29354483, -0.54591882, -1.3643328 , ..., -1.49644899,\n",
       "         -0.86241138,  5.92975044],\n",
       "        ..., \n",
       "        [-0.61501056, -0.86378241, -1.25061345, ..., -1.23034704,\n",
       "         -1.06593537,  5.30333853],\n",
       "        [-0.84677213, -0.3105891 , -1.14790535, ..., -1.31970358,\n",
       "         -0.83373034,  5.69654846],\n",
       "        [-0.45662385, -0.85049999, -1.13166726, ..., -1.64928794,\n",
       "         -1.04182208,  5.56689501]],\n",
       "\n",
       "       [[-0.28056648, -0.70264953, -1.1902833 , ..., -1.0621469 ,\n",
       "         -0.84816444,  5.25586081],\n",
       "        [-0.08658224, -1.14722431, -0.87051243, ..., -1.13106132,\n",
       "         -0.93542206,  5.28892803],\n",
       "        [ 0.03081082, -0.74203759, -1.21718454, ..., -1.41930556,\n",
       "         -0.80866522,  5.06694269],\n",
       "        ..., \n",
       "        [-0.42809474, -0.7560395 , -1.08727992, ..., -1.66476703,\n",
       "         -0.90892714,  5.25135231],\n",
       "        [-0.43014809, -0.66878307, -0.91311789, ..., -1.35964251,\n",
       "         -1.04413915,  5.29955959],\n",
       "        [-0.30437267, -0.70237374, -1.04898417, ..., -1.50949621,\n",
       "         -1.10211182,  5.00395393]],\n",
       "\n",
       "       [[-0.27031782, -0.61942595, -0.94290662, ..., -1.10398757,\n",
       "         -0.40186492,  4.33798647],\n",
       "        [-0.33244231, -0.58231366, -0.80665731, ..., -1.00703597,\n",
       "         -0.57659787,  4.24030972],\n",
       "        [-0.33371067, -0.60501707, -0.96908283, ..., -1.25415719,\n",
       "         -0.61464977,  4.14844799],\n",
       "        ..., \n",
       "        [-0.32534218, -0.38523233, -0.94320029, ..., -1.17752934,\n",
       "         -0.43306217,  4.38759851],\n",
       "        [-0.05497583, -0.676503  , -0.86848164, ..., -1.09378052,\n",
       "         -0.43537635,  3.88695478],\n",
       "        [-0.31139612, -0.33042046, -0.83733302, ..., -1.14166689,\n",
       "         -0.65296084,  4.02495575]]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preddict[\"prob\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t"
     ]
    }
   ],
   "source": [
    "for b in preddict[\"words\"]:\n",
    "    print(len(b), end=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print()\n",
    "for i in range(len(preddict[\"prob\"][0][3])):\n",
    "    print(i, alphabet[i], preddict[\"prob\"][0][3][i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
