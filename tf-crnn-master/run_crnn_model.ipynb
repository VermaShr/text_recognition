{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import argparse\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import BasicLSTMCell\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "from skimage import io as skimio\n",
    "from skimage import color as skimcolor\n",
    "import skimage.transform as skimtrans\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .decoding import get_words_from_chars\n",
    "# from .config import Params, CONST\n",
    "# from src.model import crnn_fn\n",
    "# from src.data_handler import data_loader\n",
    "# from src.data_handler import preprocess_image_for_prediction\n",
    "\n",
    "# from src.config import Params, Alphabet, import_params_from_json\n",
    "\n",
    "csv_files_train = \"../data/train.csv\"\n",
    "csv_files_eval = \"../data/valid.csv\"\n",
    "output_model_dir = \"./estimator\"\n",
    "n_epochs = 1\n",
    "gpu = \"\" # help=\"GPU 0,1 or '' \", default=''\n",
    "\n",
    "train_batch_size=64\n",
    "eval_batch_size=64\n",
    "learning_rate=1e-3  # 1e-3 recommended\n",
    "learning_decay_rate=0.95\n",
    "learning_decay_steps=5000\n",
    "evaluate_every_epoch=5\n",
    "save_interval=5e3\n",
    "input_shape=(117, 1669)\n",
    "optimizer='adam'\n",
    "digits_only=False\n",
    "alphabet=\" !\\\"#&'()*+,-./0123456789:;<=>?ABCDEFGHIJKLMNOPQRSTUVWXY[]_abcdefghijklmnopqrstuvwxyz|~\"\n",
    "alphabet_decoding='same'\n",
    "alphabet_codes = list(range(len(alphabet)))\n",
    "n_classes = len(alphabet)\n",
    "csv_delimiter='\\t'\n",
    "keep_prob_dropout = 0.7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of tensor graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed for quickly making convolutional layers\n",
    "def weightVar(shape, mean=0.0, stddev=0.02, name='weights'):\n",
    "    init_w = tf.truncated_normal(shape=shape, mean=mean, stddev=stddev)\n",
    "    return tf.Variable(init_w, name=name)\n",
    "\n",
    "\n",
    "def biasVar(shape, value=0.0, name='bias'):\n",
    "    init_b = tf.constant(value=value, shape=shape)\n",
    "    return tf.Variable(init_b, name=name)\n",
    "\n",
    "\n",
    "def conv2d(input, filter, strides=[1, 1, 1, 1], padding='SAME', name=None):\n",
    "    return tf.nn.conv2d(input, filter, strides=strides, padding=padding, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_tensor = features['images']\n",
    "input_tensor = tf.placeholder(tf.float32, [None, input_shape[0], input_shape[1], 1])\n",
    "labels = tf.placeholder(tf.string, [None])\n",
    "is_training = True\n",
    "\n",
    "if input_tensor.shape[-1] == 1:\n",
    "    input_channels = 1\n",
    "elif input_tensor.shape[-1] == 3:\n",
    "    input_channels = 3\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('deep_cnn'):\n",
    "    # conv1 - maxPool2x2\n",
    "    with tf.variable_scope('layer1'):\n",
    "        W = weightVar([3, 3, input_channels, 64])\n",
    "        b = biasVar([64])\n",
    "        conv = conv2d(input_tensor, W, name='conv')\n",
    "        out = tf.nn.bias_add(conv, b)\n",
    "        conv1 = tf.nn.relu(out)\n",
    "        pool1 = tf.nn.max_pool(conv1, [1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                               padding='SAME', name='pool')\n",
    "\n",
    "    # conv2 - maxPool 2x2\n",
    "    with tf.variable_scope('layer2'):\n",
    "        W = weightVar([3, 3, 64, 128])\n",
    "        b = biasVar([128])\n",
    "        conv = conv2d(pool1, W)\n",
    "        out = tf.nn.bias_add(conv, b)\n",
    "        conv2 = tf.nn.relu(out)\n",
    "        pool2 = tf.nn.max_pool(conv2, [1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                               padding='SAME', name='pool1')\n",
    "\n",
    "    # conv3 - w/batch-norm (as source code, not paper)\n",
    "    with tf.variable_scope('layer3'):\n",
    "        W = weightVar([3, 3, 128, 256])\n",
    "        b = biasVar([256])\n",
    "        conv = conv2d(pool2, W)\n",
    "        out = tf.nn.bias_add(conv, b)\n",
    "        b_norm = tf.layers.batch_normalization(out, axis=-1,\n",
    "                                               training=is_training, name='batch-norm')\n",
    "        conv3 = tf.nn.relu(b_norm, name='ReLU')\n",
    "\n",
    "    # conv4 - maxPool 2x1\n",
    "    with tf.variable_scope('layer4'):\n",
    "        W = weightVar([3, 3, 256, 256])\n",
    "        b = biasVar([256])\n",
    "        conv = conv2d(conv3, W)\n",
    "        out = tf.nn.bias_add(conv, b)\n",
    "        conv4 = tf.nn.relu(out)\n",
    "        pool4 = tf.nn.max_pool(conv4, [1, 2, 2, 1], strides=[1, 2, 1, 1],\n",
    "                               padding='SAME', name='pool4')\n",
    "\n",
    "    # conv5 - w/batch-norm\n",
    "    with tf.variable_scope('layer5'):\n",
    "        W = weightVar([3, 3, 256, 512])\n",
    "        b = biasVar([512])\n",
    "        conv = conv2d(pool4, W)\n",
    "        out = tf.nn.bias_add(conv, b)\n",
    "        b_norm = tf.layers.batch_normalization(out, axis=-1,\n",
    "                                               training=is_training, name='batch-norm')\n",
    "        conv5 = tf.nn.relu(b_norm)\n",
    "\n",
    "    # conv6 - maxPool 2x1 (as source code, not paper)\n",
    "    with tf.variable_scope('layer6'):\n",
    "        W = weightVar([3, 3, 512, 512])\n",
    "        b = biasVar([512])\n",
    "        conv = conv2d(conv5, W)\n",
    "        out = tf.nn.bias_add(conv, b)\n",
    "        conv6 = tf.nn.relu(out)\n",
    "        pool6 = tf.nn.max_pool(conv6, [1, 2, 2, 1], strides=[1, 2, 1, 1],\n",
    "                               padding='SAME', name='pool6')\n",
    "\n",
    "    # conv 7 - w/batch-norm (as source code, not paper)\n",
    "    with tf.variable_scope('layer7'):\n",
    "        W = weightVar([2, 2, 512, 512])\n",
    "        b = biasVar([512])\n",
    "        conv = conv2d(pool6, W, padding='VALID')\n",
    "        out = tf.nn.bias_add(conv, b)\n",
    "        b_norm = tf.layers.batch_normalization(out, axis=-1,\n",
    "                                               training=is_training, name='batch-norm')\n",
    "        conv7 = tf.nn.relu(b_norm)\n",
    "\n",
    "    # reshape output\n",
    "    with tf.variable_scope('Reshaping_cnn'):\n",
    "        shape = conv7.get_shape().as_list()  # [batch, height, width, features]\n",
    "        shape_tens = tf.shape(conv7)\n",
    "        transposed = tf.transpose(conv7, perm=[0, 2, 1, 3],\n",
    "                                  name='transposed')  # [batch, width, height, features]\n",
    "        conv_out = tf.reshape(transposed, [shape_tens[0], -1, shape[1] * shape[3]],\n",
    "                                   name='reshaped')  # [batch, width, height x features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logprob, raw_pred = deep_bidirectional_lstm(conv, params=parameters, summaries=False)\n",
    "\n",
    "# def deep_bidirectional_lstm(inputs: tf.Tensor, params: Params, summaries: bool=True) -> tf.Tensor:\n",
    "# Prepare data shape to match `bidirectional_rnn` function requirements\n",
    "# Current data input shape: (batch_size, n_steps, n_input) \"(batch, time, height)\"\n",
    "\n",
    "list_n_hidden = [256, 256]\n",
    "\n",
    "with tf.name_scope('deep_bidirectional_lstm'):\n",
    "    # Forward direction cells\n",
    "    fw_cell_list = [BasicLSTMCell(nh, forget_bias=1.0) for nh in list_n_hidden]\n",
    "    # Backward direction cells\n",
    "    bw_cell_list = [BasicLSTMCell(nh, forget_bias=1.0) for nh in list_n_hidden]\n",
    "\n",
    "    lstm_net, _, _ = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(fw_cell_list,\n",
    "                                                                    bw_cell_list,\n",
    "                                                                    conv_out, # THE INPUT\n",
    "                                                                    dtype=tf.float32\n",
    "                                                                    )\n",
    "\n",
    "    # Dropout layer\n",
    "    lstm_net = tf.nn.dropout(lstm_net, keep_prob=keep_prob_dropout)\n",
    "\n",
    "    with tf.variable_scope('Reshaping_rnn'):\n",
    "        shape = lstm_net.get_shape().as_list()  # [batch, width, 2*n_hidden]\n",
    "        rnn_reshaped = tf.reshape(lstm_net, [-1, shape[-1]])  # [batch x width, 2*n_hidden]\n",
    "\n",
    "    with tf.variable_scope('fully_connected'):\n",
    "        W = weightVar([list_n_hidden[-1]*2, n_classes])\n",
    "        b = biasVar([n_classes])\n",
    "        fc_out = tf.nn.bias_add(tf.matmul(rnn_reshaped, W), b)\n",
    "\n",
    "    shape_tens = tf.shape(lstm_net)\n",
    "    lstm_out = tf.reshape(fc_out, [shape_tens[0], -1, n_classes], name='reshape_out')  # [batch, width, n_classes]\n",
    "\n",
    "    raw_pred = tf.argmax(tf.nn.softmax(lstm_out), axis=2, name='raw_prediction')\n",
    "\n",
    "    # Swap batch and time axis\n",
    "    lstm_out = tf.transpose(lstm_out, [1, 0, 2], name='transpose_time_major')  # [width(time), batch, n_classes]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTC loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up for loss and training\n",
    "\n",
    "# Compute seq_len from image width\n",
    "n_pools = 2*2  # 2x2 pooling in dimension W on layer 1 and 2\n",
    "seq_len_inputs = tf.divide([input_shape[1]]*train_batch_size, n_pools,\n",
    "                           name='seq_len_input_op') - 1\n",
    "\n",
    "predictions_dict = {'prob': lstm_out,\n",
    "                    'raw_predictions': raw_pred,\n",
    "                    }\n",
    "\n",
    "\n",
    "# Get keys (letters) and values (integer stand ins for letters)\n",
    "# Alphabet and codes\n",
    "keys = [c for c in alphabet] # the letters themselves\n",
    "values = alphabet_codes # integer representations\n",
    "\n",
    "\n",
    "# Create non-string labels from the keys and values above\n",
    "# Convert string label to code label\n",
    "with tf.name_scope('str2code_conversion'):\n",
    "    table_str2int = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(keys, values), -1)\n",
    "    splited = tf.string_split(labels, delimiter='')  # TODO change string split to utf8 split in next tf version\n",
    "    codes = table_str2int.lookup(splited.values)\n",
    "    sparse_code_target = tf.SparseTensor(splited.indices, codes, splited.dense_shape)\n",
    "\n",
    "seq_lengths_labels = tf.bincount(tf.cast(sparse_code_target.indices[:, 0], tf.int32),\n",
    "                                 minlength=tf.shape(predictions_dict['prob'])[1])\n",
    "\n",
    "\n",
    "# Use ctc loss on probabilities from lstm output\n",
    "# Loss\n",
    "# ----\n",
    "# >>> Cannot have longer labels than predictions -> error\n",
    "with tf.control_dependencies([tf.less_equal(sparse_code_target.dense_shape[1], tf.reduce_max(tf.cast(seq_len_inputs, tf.int64)))]):\n",
    "    loss_ctc = tf.nn.ctc_loss(labels=sparse_code_target,\n",
    "                              inputs=predictions_dict['prob'],\n",
    "                              sequence_length=tf.cast(seq_len_inputs, tf.int32),\n",
    "                              preprocess_collapse_repeated=False,\n",
    "                              ctc_merge_repeated=True,\n",
    "                              ignore_longer_outputs_than_inputs=True,  # returns zero gradient in case it happens -> ema loss = NaN\n",
    "                              time_major=True)\n",
    "    loss_ctc = tf.reduce_mean(loss_ctc)\n",
    "    loss_ctc = tf.Print(loss_ctc, [loss_ctc], message='* Loss : ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTC decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_from_chars(characters_list: List[str], sequence_lengths: List[int], name='chars_conversion'):\n",
    "    with tf.name_scope(name=name):\n",
    "        def join_charcaters_fn(coords):\n",
    "            return tf.reduce_join(characters_list[coords[0]:coords[1]])\n",
    "\n",
    "        def coords_several_sequences():\n",
    "            end_coords = tf.cumsum(sequence_lengths)\n",
    "            start_coords = tf.concat([[0], end_coords[:-1]], axis=0)\n",
    "            coords = tf.stack([start_coords, end_coords], axis=1)\n",
    "            coords = tf.cast(coords, dtype=tf.int32)\n",
    "            return tf.map_fn(join_charcaters_fn, coords, dtype=tf.string)\n",
    "\n",
    "        def coords_single_sequence():\n",
    "            return tf.reduce_join(characters_list, keep_dims=True)\n",
    "\n",
    "        words = tf.cond(tf.shape(sequence_lengths)[0] > 1,\n",
    "                        true_fn=lambda: coords_several_sequences(),\n",
    "                        false_fn=lambda: coords_single_sequence())\n",
    "\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('code2str_conversion'):\n",
    "    keys = tf.cast(alphabet_codes, tf.int64)\n",
    "    values = [c for c in alphabet]\n",
    "\n",
    "    table_int2str = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(keys, values), '?')\n",
    "\n",
    "    sparse_code_pred, log_probability = tf.nn.ctc_beam_search_decoder(predictions_dict['prob'],\n",
    "                                                                      sequence_length=tf.cast(seq_len_inputs, tf.int32),\n",
    "                                                                      merge_repeated=False,\n",
    "                                                                      beam_width=100,\n",
    "                                                                      top_paths=2)\n",
    "    # Score\n",
    "    predictions_dict['score'] = tf.subtract(log_probability[:, 0], log_probability[:, 1])\n",
    "    # around 10.0 -> seems pretty sure, less than 5.0 bit unsure, some errors/challenging images\n",
    "    sparse_code_pred = sparse_code_pred[0]\n",
    "\n",
    "    sequence_lengths_pred = tf.bincount(tf.cast(sparse_code_pred.indices[:, 0], tf.int32),\n",
    "                                        minlength=tf.shape(predictions_dict['prob'])[1])\n",
    "\n",
    "    pred_chars = table_int2str.lookup(sparse_code_pred)\n",
    "    predictions_dict['words'] = get_words_from_chars(pred_chars.values, sequence_lengths=sequence_lengths_pred)\n",
    "\n",
    "    tf.summary.text('predicted_words', predictions_dict['words'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('evaluation'):\n",
    "    CER = tf.metrics.mean(tf.edit_distance(sparse_code_pred, tf.cast(sparse_code_target, dtype=tf.int64)), name='CER')\n",
    "    CER = tf.reduce_mean(tf.edit_distance(sparse_code_pred, tf.cast(sparse_code_target, dtype=tf.int64)), name='CER')\n",
    "\n",
    "    # Convert label codes to decoding alphabet to compare predicted and groundtrouth words\n",
    "    target_chars = table_int2str.lookup(tf.cast(sparse_code_target, tf.int64))\n",
    "    target_words = get_words_from_chars(target_chars.values, seq_lengths_labels)\n",
    "    accuracy = tf.metrics.accuracy(target_words, predictions_dict['words'], name='accuracy')\n",
    "\n",
    "    eval_metric_ops = {\n",
    "                       'eval/accuracy': accuracy,\n",
    "                       'eval/CER': CER,\n",
    "                       }\n",
    "    CER = tf.Print(CER, [CER], message='-- CER : ')\n",
    "    accuracy = tf.Print(accuracy, [accuracy], message='-- Accuracy : ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the learning rate as well as a moving average\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "# # Create an ExponentialMovingAverage object\n",
    "ema = tf.train.ExponentialMovingAverage(decay=0.99, num_updates=global_step, zero_debias=True)\n",
    "# Create the shadow variables, and add op to maintain moving averages\n",
    "maintain_averages_op = ema.apply([loss_ctc])\n",
    "loss_ema = ema.average(loss_ctc)\n",
    "\n",
    "# Train op\n",
    "# --------\n",
    "learning_rate = tf.train.exponential_decay(learning_rate, global_step,\n",
    "                                           learning_decay_steps, learning_decay_rate,\n",
    "                                           staircase=True)\n",
    "\n",
    "# Set up optimizer\n",
    "if optimizer == 'ada':\n",
    "    optimizer = tf.train.AdadeltaOptimizer(learning_rate)\n",
    "elif optimizer == 'adam':\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate, beta1=0.5)\n",
    "elif optimizer == 'rms':\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "opt_op = optimizer.minimize(loss_ctc, global_step=global_step)\n",
    "with tf.control_dependencies(update_ops + [opt_op]):\n",
    "    train_op = tf.group(maintain_averages_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the details to make all images the same size\n",
    "\n",
    "with open(csv_files_train, \"r\") as f:\n",
    "    f_lines = f.read().splitlines()\n",
    "    filenames = [l.split(\"\\t\")[0] for l in f_lines]\n",
    "    label_list = [l.split(\"\\t\")[1][1:-1] for l in f_lines]\n",
    "datasize = len(label_list)\n",
    "    \n",
    "def _parse_function(filename, label):\n",
    "    image_string = tf.read_file(filename)\n",
    "    image_decoded = tf.image.decode_png(image_string, channels=1)\n",
    "    image_resized = tf.image.resize_images(image_decoded, input_shape)\n",
    "    return image_decoded, label\n",
    "    \n",
    "dataset = tf.data.Dataset.from_tensor_slices((filenames, label_list))\n",
    "dataset = dataset.map(_parse_function).batch(train_batch_size)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_batch = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "#     saver.restore(sess, output_model_dir+\"/model0.ckpt\")\n",
    "    \n",
    "    # writer = tf.summary.FileWriter('./my_graph/LogRegNormal', sess.graph)\n",
    "    n_batches = int(datasize / train_batch_size)\n",
    "    for i in range(n_epochs):\n",
    "        print(\"Starting epoch\", i)\n",
    "        sess.run(iterator.initializer)\n",
    "        for j in range(n_batches):\n",
    "            input_tensor_b, labels_b = sess.run(next_batch)\n",
    "            _, cer, acc, loss, preddict, scp, pc = sess.run([train_op, CER, accuracy, loss_ctc, predictions_dict,\n",
    "                                                          sparse_code_pred, pred_chars],\n",
    "                         feed_dict={input_tensor: input_tensor_b, labels: labels_b})\n",
    "            pickle.dump([cer, acc, loss, preddict, scp, pc], open(output_model_dir+\"/preds.pkl\", \"wb\"))\n",
    "#             _, cer, acc, loss = sess.run([train_op, CER, accuracy, loss_ctc],\n",
    "#                          feed_dict={input_tensor: input_tensor_b, labels: labels_b})\n",
    "            saver.save(sess, output_model_dir+\"/model0.ckpt\")\n",
    "            \n",
    "            print('batch: {0}, CER: {1}, accuracy: {2}, loss: {3}'.format(j, cer, acc, loss))\n",
    "\n",
    "    print('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "    print('Optimization Finished!') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try getting actual predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./estimator/AAAAmodel4.ckpt\n",
      "Starting epoch 0\n",
      "batch: 0, CER: 0.8816022872924805, accuracy: [0. 0.], loss: 167.15176391601562\n",
      "batch: 1, CER: 0.8727513551712036, accuracy: [0. 0.], loss: 169.19094848632812\n",
      "Total time: 118.00451493263245 seconds\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    saver.restore(sess, output_model_dir+\"/AAAAmodel4.ckpt\")\n",
    "    start_time = time.time()\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # writer = tf.summary.FileWriter('./my_graph/LogRegNormal', sess.graph)\n",
    "    n_batches = int(datasize / train_batch_size)\n",
    "    for i in range(n_epochs):\n",
    "        print(\"Starting epoch\", i)\n",
    "        sess.run(iterator.initializer)\n",
    "        for j in range(n_batches):\n",
    "            input_tensor_b, labels_b = sess.run(next_batch)\n",
    "            cer, acc, loss, preddict, scp, pc = sess.run([CER, accuracy, loss_ctc, predictions_dict,\n",
    "                                                          sparse_code_pred, pred_chars],\n",
    "                         feed_dict={input_tensor: input_tensor_b, labels: labels_b})\n",
    "            \n",
    "            pickle.dump([cer, acc, loss, preddict, scp, pc], open(output_model_dir+\"/preds2.pkl\", \"wb\"))\n",
    "            print('batch: {0}, CER: {1}, accuracy: {2}, loss: {3}'.format(j, cer, acc, loss))\n",
    "            if j > 0: break # for now don't do all of them\n",
    "\n",
    "    print('Total time: {0} seconds'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-7cf8e452113d>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-7cf8e452113d>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    preddict[]\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "preddict[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\t8\t7\t8\t8\t6\t8\t7\t4\t8\t6\t2\t9\t9\t7\t9\t9\t6\t8\t10\t4\t9\t9\t8\t8\t8\t8\t8\t5\t3\t8\t3\t6\t7\t9\t10\t9\t6\t9\t8\t2\t5\t6\t6\t7\t6\t5\t7\t7\t8\t6\t3\t8\t5\t9\t7\t8\t8\t6\t7\t7\t7\t8\t8\t"
     ]
    }
   ],
   "source": [
    "for b in preddict[\"words\"]:\n",
    "    print(len(b), end=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "for i in range(len(preddict[\"prob\"][0][3])):\n",
    "    print(i, alphabet[i], preddict[\"prob\"][0][3][i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
